{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80469363",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import HDBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import gaussian_kde\n",
    "import os\n",
    "import itertools\n",
    "import warnings\n",
    "import math\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e27e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_filename = 'all_colors_data_optimal_embeddings.csv'\n",
    "results_name = 'all_colors_data'\n",
    "base_path = r'C:\\Users\\shash\\OneDrive\\Documents\\Dessertation\\Astro-DIM\\data\\processed'\n",
    "embeddings_filepath = r'C:\\Users\\shash\\OneDrive\\Documents\\Dessertation\\Astro-DIM\\results\\all colors data'\n",
    "results_path = r'C:\\Users\\shash\\OneDrive\\Documents\\Dessertation\\Astro-DIM\\results\\all colors data\\clustering'\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "PLOTS_PER_PAGE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb90f29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loading combined embeddings data from: {embeddings_filename}\")\n",
    "df_embeddings = pd.read_csv(os.path.join(embeddings_filepath, embeddings_filename))\n",
    "\n",
    "print(f\"Data shape: {df_embeddings.shape}\")\n",
    "print(f\"Columns: {list(df_embeddings.columns)}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df_embeddings.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7a620f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_ids = df_embeddings['sample_id'].values\n",
    "\n",
    "feature_cols = [col for col in df_embeddings.columns if col.startswith('feature_')]\n",
    "features = df_embeddings[feature_cols].values if feature_cols else None\n",
    "\n",
    "pca_cols = [col for col in df_embeddings.columns if col.startswith('PCA_')]\n",
    "pca_data = df_embeddings[pca_cols].values if pca_cols else None\n",
    "\n",
    "ae_cols = [col for col in df_embeddings.columns if col.startswith('AE_')]\n",
    "ae_data = df_embeddings[ae_cols].values if ae_cols else None\n",
    "\n",
    "print(f\"\\nDetected optimal embedding spaces:\")\n",
    "if features is not None:\n",
    "    print(f\"Original features: {features.shape} - {len(feature_cols)} features\")\n",
    "if pca_data is not None:\n",
    "    print(f\"Optimal PCA embeddings: {pca_data.shape} - {len(pca_cols)} components\")\n",
    "if ae_data is not None:\n",
    "    print(f\"Optimal AE embeddings: {ae_data.shape} - {len(ae_cols)} components\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b298de08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_hdbscan_all_combinations(X_data, space_name=\"\", verbose=True):\n",
    "    \n",
    "    min_cluster_sizes = [5]\n",
    "    min_samples_list = [5]\n",
    "    cluster_selection_epsilons = [2.0]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nRunning HDBSCAN clustering for {space_name}\")\n",
    "        total_combinations = len(min_cluster_sizes) * len(min_samples_list) * len(cluster_selection_epsilons)\n",
    "        print(f\"Parameter combinations: {len(min_cluster_sizes)} × {len(min_samples_list)} × {len(cluster_selection_epsilons)} = {total_combinations}\")\n",
    "        print(f\"Data shape: {X_data.shape}\")\n",
    "        print(\"-\" * 80)\n",
    "    \n",
    "    combination_count = 0\n",
    "    \n",
    "    for min_cluster_size in min_cluster_sizes:\n",
    "        for min_samples in min_samples_list:\n",
    "            for cluster_selection_epsilon in cluster_selection_epsilons:\n",
    "                combination_count += 1\n",
    "                \n",
    "                try:\n",
    "                    clusterer = HDBSCAN(\n",
    "                        min_cluster_size=min_cluster_size,\n",
    "                        min_samples=min_samples,\n",
    "                        metric='euclidean',\n",
    "                        cluster_selection_method='eom',\n",
    "                        cluster_selection_epsilon=cluster_selection_epsilon\n",
    "                    )\n",
    "                    \n",
    "                    labels = clusterer.fit_predict(X_data)\n",
    "                    \n",
    "                    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "                    n_noise = np.sum(labels == -1)\n",
    "                    noise_fraction = n_noise / len(labels)\n",
    "                    \n",
    "                    result = {\n",
    "                        'combination_id': combination_count,\n",
    "                        'min_cluster_size': min_cluster_size,\n",
    "                        'min_samples': min_samples,\n",
    "                        'cluster_selection_epsilon': cluster_selection_epsilon,\n",
    "                        'n_clusters': n_clusters,\n",
    "                        'n_noise': int(n_noise),\n",
    "                        'noise_fraction': noise_fraction,\n",
    "                        'labels': labels.copy()\n",
    "                    }\n",
    "                    \n",
    "                    results.append(result)\n",
    "                    \n",
    "                    if verbose:\n",
    "                        print(f\"{combination_count:2d}. mcs={min_cluster_size:>2} ms={min_samples:>2} eps={cluster_selection_epsilon:>3.1f} | clusters={n_clusters:>2} noise={noise_fraction:>5.1%}\")\n",
    "                \n",
    "                except Exception as e:\n",
    "                    if verbose:\n",
    "                        print(f\"{combination_count:2d}. Error with mcs={min_cluster_size} ms={min_samples} eps={cluster_selection_epsilon}: {e}\")\n",
    "                    continue\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"-\" * 80)\n",
    "        print(f\"Completed {len(results)} successful clustering runs\")\n",
    "    \n",
    "    results_data = []\n",
    "    for result in results:\n",
    "        results_data.append({\n",
    "            'combination_id': result['combination_id'],\n",
    "            'min_cluster_size': result['min_cluster_size'],\n",
    "            'min_samples': result['min_samples'],\n",
    "            'cluster_selection_epsilon': result['cluster_selection_epsilon'],\n",
    "            'n_clusters': result['n_clusters'],\n",
    "            'n_noise': result['n_noise'],\n",
    "            'noise_fraction': result['noise_fraction']\n",
    "        })\n",
    "    \n",
    "    if results_data:\n",
    "        results_df = pd.DataFrame(results_data)\n",
    "        space_clean = space_name.lower().replace(' ', '_').replace('(', '').replace(')', '')\n",
    "        results_filename = f'{results_name}_{space_clean}_hdbscan_results.csv'\n",
    "        results_df.to_csv(os.path.join(results_path, results_filename), index=False)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Results saved to: {results_filename}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ca6858",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cluster_specific_plots(X_data, labels, space_name, results_path, results_name, \n",
    "                                 feature_names=None, combination_info=None, plots_per_page=16):\n",
    "    \n",
    "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    n_noise = np.sum(labels == -1)\n",
    "    noise_fraction = n_noise / len(labels)\n",
    "    \n",
    "    if combination_info:\n",
    "        param_text = f\"mcs={combination_info['min_cluster_size']} ms={combination_info['min_samples']} eps={combination_info['cluster_selection_epsilon']}\"\n",
    "        combination_id = combination_info['combination_id']\n",
    "    else:\n",
    "        param_text = \"\"\n",
    "        combination_id = \"unknown\"\n",
    "    \n",
    "    n_dims = X_data.shape[1]\n",
    "    \n",
    "    dim_pairs = list(itertools.combinations(range(n_dims), 2))\n",
    "    total_pairs = len(dim_pairs)\n",
    "    \n",
    "    print(f\"Total dimension pairs: {total_pairs}\")\n",
    "    print(f\"Plots per page: {plots_per_page}\")\n",
    "    \n",
    "    grid_cols = int(math.sqrt(plots_per_page))\n",
    "    grid_rows = math.ceil(plots_per_page / grid_cols)\n",
    "    \n",
    "    pages = [dim_pairs[i:i+plots_per_page] for i in range(0, len(dim_pairs), plots_per_page)]\n",
    "    \n",
    "    print(f\"Will create {len(pages)} pages with {grid_rows}x{grid_cols} grids\")\n",
    "    \n",
    "    unique_labels = np.unique(labels)\n",
    "    \n",
    "    for cluster_label in unique_labels:\n",
    "        if cluster_label >= 0:\n",
    "            print(f\"\\nCreating plots for Cluster {cluster_label}...\")\n",
    "            for page_num, page_pairs in enumerate(pages, 1):\n",
    "                create_single_cluster_page(X_data, labels, page_pairs, space_name, results_path, \n",
    "                                         results_name, feature_names, combination_info, \n",
    "                                         cluster_label, page_num, len(pages), grid_rows, grid_cols)\n",
    "    \n",
    "    if -1 in unique_labels:\n",
    "        print(f\"\\nCreating plots for Noise points...\")\n",
    "        for page_num, page_pairs in enumerate(pages, 1):\n",
    "            create_noise_page(X_data, labels, page_pairs, space_name, results_path, \n",
    "                            results_name, feature_names, combination_info, \n",
    "                            page_num, len(pages), grid_rows, grid_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77166eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_single_cluster_page(X_data, labels, dim_pairs, space_name, results_path, results_name,\n",
    "                              feature_names, combination_info, cluster_label, page_num, total_pages, \n",
    "                              grid_rows, grid_cols):\n",
    "    \n",
    "    combination_id = combination_info['combination_id'] if combination_info else \"unknown\"\n",
    "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    noise_fraction = np.sum(labels == -1) / len(labels)\n",
    "    param_text = f\"mcs={combination_info['min_cluster_size']} ms={combination_info['min_samples']} eps={combination_info['cluster_selection_epsilon']}\" if combination_info else \"\"\n",
    "    \n",
    "    cluster_mask = labels == cluster_label\n",
    "    cluster_size = np.sum(cluster_mask)\n",
    "    \n",
    "    fig, axes = plt.subplots(grid_rows, grid_cols, figsize=(4*grid_cols, 3*grid_rows))\n",
    "    if len(dim_pairs) == 1:\n",
    "        axes = [axes]\n",
    "    else:\n",
    "        axes = axes.flatten() if len(dim_pairs) > 1 else [axes]\n",
    "    \n",
    "    main_title = f'{space_name} - CLUSTER {cluster_label} - Page {page_num}/{total_pages}\\nCombo {combination_id}: {cluster_size} points | {param_text}'\n",
    "    \n",
    "    for idx, (dim1, dim2) in enumerate(dim_pairs):\n",
    "        if idx >= len(axes):\n",
    "            break\n",
    "            \n",
    "        ax = axes[idx]\n",
    "        x, y = X_data[:, dim1], X_data[:, dim2]\n",
    "        \n",
    "        ax.scatter(x, y, c='lightgray', alpha=0.2, s=0.5, label='Other data')\n",
    "        \n",
    "        cluster_x, cluster_y = x[cluster_mask], y[cluster_mask]\n",
    "        ax.scatter(cluster_x, cluster_y, c='red', alpha=0.8, s=3, label=f'Cluster {cluster_label}')\n",
    "        \n",
    "        if feature_names and len(feature_names) > max(dim1, dim2):\n",
    "            xlabel = feature_names[dim1].replace('feature_', '')\n",
    "            ylabel = feature_names[dim2].replace('feature_', '')\n",
    "        else:\n",
    "            xlabel = f'Dim {dim1+1}'\n",
    "            ylabel = f'Dim {dim2+1}'\n",
    "            \n",
    "        ax.set_xlabel(xlabel, fontsize=8)\n",
    "        ax.set_ylabel(ylabel, fontsize=8)\n",
    "        ax.set_title(f'Dims {dim1+1}-{dim2+1}', fontsize=9)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    for idx in range(len(dim_pairs), len(axes)):\n",
    "        fig.delaxes(axes[idx])\n",
    "    \n",
    "    plt.suptitle(main_title, fontsize=10, y=0.98)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    \n",
    "    space_clean = space_name.lower().replace(' ', '_').replace('(', '').replace(')', '')\n",
    "    filename = f'{results_name}_{space_clean}_combo_{combination_id}_cluster_{cluster_label}_page_{page_num:02d}.png'\n",
    "    plt.savefig(os.path.join(results_path, filename), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"Saved: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e43c1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_noise_page(X_data, labels, dim_pairs, space_name, results_path, results_name,\n",
    "                     feature_names, combination_info, page_num, total_pages, grid_rows, grid_cols):\n",
    "    \n",
    "    combination_id = combination_info['combination_id'] if combination_info else \"unknown\"\n",
    "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    noise_fraction = np.sum(labels == -1) / len(labels)\n",
    "    param_text = f\"mcs={combination_info['min_cluster_size']} ms={combination_info['min_samples']} eps={combination_info['cluster_selection_epsilon']}\" if combination_info else \"\"\n",
    "    \n",
    "    noise_mask = labels == -1\n",
    "    noise_size = np.sum(noise_mask)\n",
    "    \n",
    "    fig, axes = plt.subplots(grid_rows, grid_cols, figsize=(4*grid_cols, 3*grid_rows))\n",
    "    if len(dim_pairs) == 1:\n",
    "        axes = [axes]\n",
    "    else:\n",
    "        axes = axes.flatten() if len(dim_pairs) > 1 else [axes]\n",
    "    \n",
    "    main_title = f'{space_name} - NOISE POINTS - Page {page_num}/{total_pages}\\nCombo {combination_id}: {noise_size} points | {param_text}'\n",
    "    \n",
    "    for idx, (dim1, dim2) in enumerate(dim_pairs):\n",
    "        if idx >= len(axes):\n",
    "            break\n",
    "            \n",
    "        ax = axes[idx]\n",
    "        x, y = X_data[:, dim1], X_data[:, dim2]\n",
    "        \n",
    "        ax.scatter(x, y, c='lightgray', alpha=0.2, s=0.5, label='Other data')\n",
    "        \n",
    "        noise_x, noise_y = x[noise_mask], y[noise_mask]\n",
    "        ax.scatter(noise_x, noise_y, c='black', alpha=0.6, s=1, label='Noise')\n",
    "        \n",
    "        if feature_names and len(feature_names) > max(dim1, dim2):\n",
    "            xlabel = feature_names[dim1].replace('feature_', '')\n",
    "            ylabel = feature_names[dim2].replace('feature_', '')\n",
    "        else:\n",
    "            xlabel = f'Dim {dim1+1}'\n",
    "            ylabel = f'Dim {dim2+1}'\n",
    "            \n",
    "        ax.set_xlabel(xlabel, fontsize=8)\n",
    "        ax.set_ylabel(ylabel, fontsize=8)\n",
    "        ax.set_title(f'Dims {dim1+1}-{dim2+1}', fontsize=9)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    for idx in range(len(dim_pairs), len(axes)):\n",
    "        fig.delaxes(axes[idx])\n",
    "    \n",
    "    plt.suptitle(main_title, fontsize=10, y=0.98)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    \n",
    "    space_clean = space_name.lower().replace(' ', '_').replace('(', '').replace(')', '')\n",
    "    filename = f'{results_name}_{space_clean}_combo_{combination_id}_noise_page_{page_num:02d}.png'\n",
    "    plt.savefig(os.path.join(results_path, filename), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"Saved: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ab28b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_clustering_plots_all_pairs(X_data, labels, space_name, results_path, results_name, \n",
    "                                     feature_names=None, combination_info=None, plots_per_page=16):\n",
    "    \n",
    "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    n_noise = np.sum(labels == -1)\n",
    "    noise_fraction = n_noise / len(labels)\n",
    "    \n",
    "    if combination_info:\n",
    "        param_text = f\"mcs={combination_info['min_cluster_size']} ms={combination_info['min_samples']} eps={combination_info['cluster_selection_epsilon']}\"\n",
    "        combination_id = combination_info['combination_id']\n",
    "    else:\n",
    "        param_text = \"\"\n",
    "        combination_id = \"unknown\"\n",
    "    \n",
    "    n_dims = X_data.shape[1]\n",
    "    \n",
    "    dim_pairs = list(itertools.combinations(range(n_dims), 2))\n",
    "    total_pairs = len(dim_pairs)\n",
    "    \n",
    "    print(f\"Total dimension pairs: {total_pairs}\")\n",
    "    print(f\"Plots per page: {plots_per_page}\")\n",
    "    \n",
    "    grid_cols = int(math.sqrt(plots_per_page))\n",
    "    grid_rows = math.ceil(plots_per_page / grid_cols)\n",
    "    \n",
    "    pages = [dim_pairs[i:i+plots_per_page] for i in range(0, len(dim_pairs), plots_per_page)]\n",
    "    \n",
    "    print(f\"Will create {len(pages)} pages with {grid_rows}x{grid_cols} grids\")\n",
    "    \n",
    "    for page_num, page_pairs in enumerate(pages, 1):\n",
    "        create_single_page_scatter(X_data, labels, page_pairs, space_name, results_path, \n",
    "                                 results_name, feature_names, combination_info, \n",
    "                                 page_num, len(pages), grid_rows, grid_cols)\n",
    "    \n",
    "    for page_num, page_pairs in enumerate(pages, 1):\n",
    "        create_single_page_contour(X_data, labels, page_pairs, space_name, results_path, \n",
    "                                 results_name, feature_names, combination_info, \n",
    "                                 page_num, len(pages), grid_rows, grid_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936bef35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_single_page_scatter(X_data, labels, dim_pairs, space_name, results_path, results_name,\n",
    "                              feature_names, combination_info, page_num, total_pages, grid_rows, grid_cols):\n",
    "    \n",
    "    combination_id = combination_info['combination_id'] if combination_info else \"unknown\"\n",
    "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    noise_fraction = np.sum(labels == -1) / len(labels)\n",
    "    param_text = f\"mcs={combination_info['min_cluster_size']} ms={combination_info['min_samples']} eps={combination_info['cluster_selection_epsilon']}\" if combination_info else \"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(grid_rows, grid_cols, figsize=(4*grid_cols, 3*grid_rows))\n",
    "    if len(dim_pairs) == 1:\n",
    "        axes = [axes]\n",
    "    else:\n",
    "        axes = axes.flatten() if len(dim_pairs) > 1 else [axes]\n",
    "    \n",
    "    main_title = f'{space_name} - SCATTER - Page {page_num}/{total_pages}\\nCombo {combination_id}: {n_clusters} Clusters, {noise_fraction:.1%} Noise | {param_text}'\n",
    "    \n",
    "    for idx, (dim1, dim2) in enumerate(dim_pairs):\n",
    "        if idx >= len(axes):\n",
    "            break\n",
    "            \n",
    "        ax = axes[idx]\n",
    "        x, y = X_data[:, dim1], X_data[:, dim2]\n",
    "        \n",
    "        colors = labels.copy().astype(float)\n",
    "        colors[labels == -1] = -0.5\n",
    "        \n",
    "        scatter = ax.scatter(x, y, c=colors, cmap='viridis', alpha=0.6, s=1)\n",
    "        \n",
    "        if feature_names and len(feature_names) > max(dim1, dim2):\n",
    "            xlabel = feature_names[dim1].replace('feature_', '')\n",
    "            ylabel = feature_names[dim2].replace('feature_', '')\n",
    "        else:\n",
    "            xlabel = f'Dim {dim1+1}'\n",
    "            ylabel = f'Dim {dim2+1}'\n",
    "            \n",
    "        ax.set_xlabel(xlabel, fontsize=8)\n",
    "        ax.set_ylabel(ylabel, fontsize=8)\n",
    "        ax.set_title(f'Dims {dim1+1}-{dim2+1}', fontsize=9)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    for idx in range(len(dim_pairs), len(axes)):\n",
    "        fig.delaxes(axes[idx])\n",
    "    \n",
    "    plt.suptitle(main_title, fontsize=10, y=0.98)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    \n",
    "    space_clean = space_name.lower().replace(' ', '_').replace('(', '').replace(')', '')\n",
    "    filename = f'{results_name}_{space_clean}_combo_{combination_id}_scatter_page_{page_num:02d}.png'\n",
    "    plt.savefig(os.path.join(results_path, filename), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"Saved: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b64f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_single_page_contour(X_data, labels, dim_pairs, space_name, results_path, results_name,\n",
    "                              feature_names, combination_info, page_num, total_pages, grid_rows, grid_cols):\n",
    "    \n",
    "    combination_id = combination_info['combination_id'] if combination_info else \"unknown\"\n",
    "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    noise_fraction = np.sum(labels == -1) / len(labels)\n",
    "    param_text = f\"mcs={combination_info['min_cluster_size']} ms={combination_info['min_samples']} eps={combination_info['cluster_selection_epsilon']}\" if combination_info else \"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(grid_rows, grid_cols, figsize=(4*grid_cols, 3*grid_rows))\n",
    "    if len(dim_pairs) == 1:\n",
    "        axes = [axes]\n",
    "    else:\n",
    "        axes = axes.flatten() if len(dim_pairs) > 1 else [axes]\n",
    "    \n",
    "    main_title = f'{space_name} - CONTOUR - Page {page_num}/{total_pages}\\nCombo {combination_id}: {n_clusters} Clusters, {noise_fraction:.1%} Noise | {param_text}'\n",
    "    \n",
    "    for idx, (dim1, dim2) in enumerate(dim_pairs):\n",
    "        if idx >= len(axes):\n",
    "            break\n",
    "            \n",
    "        ax = axes[idx]\n",
    "        x, y = X_data[:, dim1], X_data[:, dim2]\n",
    "        \n",
    "        try:\n",
    "            unique_labels = np.unique(labels[labels >= 0])\n",
    "            colors = plt.cm.Set1(np.linspace(0, 1, len(unique_labels)))\n",
    "            \n",
    "            for i, label in enumerate(unique_labels):\n",
    "                cluster_mask = labels == label\n",
    "                cluster_points = np.sum(cluster_mask)\n",
    "                \n",
    "                cluster_x, cluster_y = x[cluster_mask], y[cluster_mask]\n",
    "                \n",
    "                if cluster_points < 3:\n",
    "                    ax.scatter(cluster_x, cluster_y, c=[colors[i]], s=50, \n",
    "                                    alpha=0.8, marker='o', edgecolors='black', linewidth=0.5)\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    if cluster_points <= 5:\n",
    "                        bw_method = 1.0\n",
    "                    elif cluster_points <= 10:\n",
    "                        bw_method = 0.8\n",
    "                    elif cluster_points <= 20:\n",
    "                        bw_method = 0.6\n",
    "                    else:\n",
    "                        bw_method = 'scott'\n",
    "                    \n",
    "                    kde = gaussian_kde(np.vstack([cluster_x, cluster_y]), bw_method=bw_method)\n",
    "                    \n",
    "                    x_min, x_max = x.min() - 0.1*(x.max()-x.min()), x.max() + 0.1*(x.max()-x.min())\n",
    "                    y_min, y_max = y.min() - 0.1*(y.max()-y.min()), y.max() + 0.1*(y.max()-y.min())\n",
    "                    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 50), np.linspace(y_min, y_max, 50))\n",
    "                    \n",
    "                    density = kde(np.vstack([xx.ravel(), yy.ravel()])).reshape(xx.shape)\n",
    "                    \n",
    "                    if np.isfinite(density).all() and np.max(density) > np.min(density):\n",
    "                        levels = 3 if cluster_points <= 10 else 5\n",
    "                        ax.contour(xx, yy, density, levels=levels, \n",
    "                                        colors=[colors[i]], linewidths=1.0, alpha=0.7)\n",
    "                    else:\n",
    "                        ax.scatter(cluster_x, cluster_y, c=[colors[i]], s=20, \n",
    "                                        alpha=0.8, marker='^', edgecolors='black', linewidth=0.5)\n",
    "                    \n",
    "                except:\n",
    "                    ax.scatter(cluster_x, cluster_y, c=[colors[i]], s=15, \n",
    "                                    alpha=0.8, marker='d', edgecolors='black', linewidth=0.5)\n",
    "            \n",
    "            if feature_names and len(feature_names) > max(dim1, dim2):\n",
    "                xlabel = feature_names[dim1].replace('feature_', '')\n",
    "                ylabel = feature_names[dim2].replace('feature_', '')\n",
    "            else:\n",
    "                xlabel = f'Dim {dim1+1}'\n",
    "                ylabel = f'Dim {dim2+1}'\n",
    "                \n",
    "            ax.set_xlabel(xlabel, fontsize=8)\n",
    "            ax.set_ylabel(ylabel, fontsize=8)\n",
    "            ax.set_title(f'Dims {dim1+1}-{dim2+1}', fontsize=9)\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            \n",
    "        except Exception as e:\n",
    "            ax.text(0.5, 0.5, 'Visualization\\nnot available', ha='center', va='center', \n",
    "                          transform=ax.transAxes, fontsize=10, alpha=0.7)\n",
    "            ax.set_title(f'Dims {dim1+1}-{dim2+1}', fontsize=9)\n",
    "    \n",
    "    for idx in range(len(dim_pairs), len(axes)):\n",
    "        fig.delaxes(axes[idx])\n",
    "    \n",
    "    plt.suptitle(main_title, fontsize=10, y=0.98)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    \n",
    "    space_clean = space_name.lower().replace(' ', '_').replace('(', '').replace(')', '')\n",
    "    filename = f'{results_name}_{space_clean}_combo_{combination_id}_contour_page_{page_num:02d}.png'\n",
    "    plt.savefig(os.path.join(results_path, filename), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"Saved: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acc2cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spaces = {}\n",
    "\n",
    "if features is not None:\n",
    "    spaces['Original Features'] = {\n",
    "        'data': features,\n",
    "        'feature_names': [col.replace('feature_', '') for col in feature_cols]\n",
    "    }\n",
    "\n",
    "if pca_data is not None:\n",
    "    space_name = f'Optimal PCA Space ({pca_data.shape[1]}D)'\n",
    "    spaces[space_name] = {\n",
    "        'data': pca_data,\n",
    "        'feature_names': pca_cols\n",
    "    }\n",
    "\n",
    "if ae_data is not None:\n",
    "    space_name = f'Optimal AE Space ({ae_data.shape[1]}D)'\n",
    "    spaces[space_name] = {\n",
    "        'data': ae_data,\n",
    "        'feature_names': ae_cols\n",
    "    }\n",
    "\n",
    "print(f\"\\nOptimal spaces to analyze: {list(spaces.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161c0ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = {}\n",
    "\n",
    "for space_name, space_info in spaces.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ANALYZING: {space_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    X_data = space_info['data']\n",
    "    feature_names = space_info['feature_names']\n",
    "    \n",
    "    if np.isnan(X_data).any():\n",
    "        print(f\"Warning: NaN values found in {space_name}, skipping...\")\n",
    "        continue\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_data)\n",
    "    \n",
    "    clustering_results = run_hdbscan_all_combinations(X_scaled, space_name, verbose=True)\n",
    "    \n",
    "    all_results[space_name] = clustering_results\n",
    "    \n",
    "    print(f\"\\nCreating plots for {len(clustering_results)} combinations...\")\n",
    "    \n",
    "    for result in clustering_results:\n",
    "        labels = result['labels']\n",
    "        combination_info = {\n",
    "            'combination_id': result['combination_id'],\n",
    "            'min_cluster_size': result['min_cluster_size'],\n",
    "            'min_samples': result['min_samples'],\n",
    "            'cluster_selection_epsilon': result['cluster_selection_epsilon']\n",
    "        }\n",
    "        \n",
    "        # Create cluster-specific plots \n",
    "        create_cluster_specific_plots(X_scaled, labels, space_name, results_path, \n",
    "                                    results_name, feature_names, combination_info, PLOTS_PER_PAGE)\n",
    "        \n",
    "        # Save cluster labels\n",
    "        labels_df = pd.DataFrame({\n",
    "            'sample_id': sample_ids,\n",
    "            'cluster_labels': labels\n",
    "        })\n",
    "        \n",
    "        space_clean = space_name.lower().replace(' ', '_').replace('(', '').replace(')', '')\n",
    "        labels_filename = f'{results_name}_{space_clean}_combo_{result[\"combination_id\"]}_labels.csv'\n",
    "        labels_df.to_csv(os.path.join(results_path, labels_filename), index=False)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"CLUSTERING ANALYSIS COMPLETE\")\n",
    "print(f\"{'='*60}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
