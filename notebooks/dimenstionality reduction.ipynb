{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bbf66f4",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a843ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "import os\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8928cc82",
   "metadata": {},
   "source": [
    "## Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ebffcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_file = 'all_colors_data.csv'\n",
    "results_name = 'all_colors_data'\n",
    "\n",
    "base_path = r'C:\\Users\\shash\\OneDrive\\Documents\\Dessertation\\Astro-DIM\\data\\processed'\n",
    "results_path = r'C:\\Users\\shash\\OneDrive\\Documents\\Dessertation\\Astro-DIM\\results\\all colors data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5be9022",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75f06bd",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4b5990",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loading {dataset_file}...\")\n",
    "df = pd.read_csv(os.path.join(base_path, dataset_file))\n",
    "\n",
    "if df.isna().any().any():\n",
    "    df = df.dropna()\n",
    "\n",
    "sample_size = 100000\n",
    "if len(df) > sample_size:\n",
    "    df = df.sample(n=sample_size, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2920fb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df, X_tmp_df = train_test_split(\n",
    "    df, train_size=50_000, shuffle=True, random_state=42\n",
    ")\n",
    "X_val_df, X_test_df = train_test_split(\n",
    "    X_tmp_df, train_size=25_000, test_size=25_000, shuffle=True, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1924531a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(X_train_df)\n",
    "X_train = scaler.transform(X_train_df)\n",
    "X_val = scaler.transform(X_val_df)\n",
    "X_test = scaler.transform(X_test_df)\n",
    "\n",
    "print(f\"Train shape: {X_train.shape}\")\n",
    "print(f\"Val shape: {X_val.shape}\")\n",
    "print(f\"Test shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26df432d",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = list(df.columns)\n",
    "print(f\"Features: {feature_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d7313e",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a60ad18",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_full = PCA()\n",
    "pca_full.fit(X_train)\n",
    "\n",
    "explained_variance = pca_full.explained_variance_ratio_\n",
    "cumulative_variance = np.cumsum(explained_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d44851b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components_95 = np.argmax(cumulative_variance >= 0.95) + 1\n",
    "print(f\"Number of components for 95% variance: {n_components_95}\")\n",
    "\n",
    "results = []\n",
    "for i, (individual, cumulative) in enumerate(zip(explained_variance, cumulative_variance)):\n",
    "    results.append({\n",
    "        'component': i + 1,\n",
    "        'explained_variance_ratio': individual,\n",
    "        'cumulative_variance': cumulative\n",
    "    })\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results.to_csv(os.path.join(results_path, f'{results_name}_pca_results.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6b9f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_optimal = PCA(n_components=n_components_95)\n",
    "pca_optimal.fit(X_train)\n",
    "X_test_pca_optimal = pca_optimal.transform(X_test)\n",
    "X_train_pca_optimal = pca_optimal.transform(X_train)\n",
    "X_val_pca_optimal = pca_optimal.transform(X_val)\n",
    "\n",
    "print(f\"Generated PCA embeddings for optimal {n_components_95} components\")\n",
    "optimal_pca_embedding = X_test_pca_optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474ddc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "axes[0].bar(range(1, min(11, len(explained_variance)+1)), explained_variance[:10])\n",
    "axes[0].set_title('Individual Variance Explained (First 10 PCs)')\n",
    "axes[0].set_xlabel('Principal Component')\n",
    "axes[0].set_ylabel('Variance Explained')\n",
    "\n",
    "axes[1].plot(range(1, len(cumulative_variance)+1), cumulative_variance, marker='o')\n",
    "axes[1].axhline(y=0.95, color='red', linestyle='--', label='95%')\n",
    "axes[1].axvline(x=n_components_95, color='green', linestyle='--', label=f'Optimal: {n_components_95}D')\n",
    "axes[1].set_title('Cumulative Variance Explained')\n",
    "axes[1].set_xlabel('Principal Component')\n",
    "axes[1].set_ylabel('Cumulative Variance')\n",
    "axes[1].set_xticks(range(1, len(cumulative_variance)+1))\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(results_path, f'{results_name}_pca_variance_plot.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8418a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "components = pca_optimal.components_\n",
    "components_df = pd.DataFrame(\n",
    "    components.T,\n",
    "    columns=[f'PC{i+1}' for i in range(components.shape[0])],\n",
    "    index=feature_names\n",
    ")\n",
    "components_df.to_csv(os.path.join(results_path, f'{results_name}_optimal_loadings.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6baff456",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(max(10, n_components_95*2), 8))\n",
    "sns.heatmap(components_df, annot=True, cmap='RdBu_r', center=0, \n",
    "            fmt='.3f', cbar_kws={'label': 'Loading Value'})\n",
    "plt.title(f'Optimal PCA Component Loadings ({n_components_95}D)')\n",
    "plt.xlabel('Principal Components')\n",
    "plt.ylabel('Original Features')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(results_path, f'{results_name}_optimal_loadings_heatmap.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa02d161",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = len(feature_names)\n",
    "\n",
    "for pc_idx in range(n_components_95):\n",
    "    n_cols = 4\n",
    "    n_rows = (n_features + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(4*n_cols, 3*n_rows))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for feat_idx in range(n_features):\n",
    "        ax = axes[feat_idx]\n",
    "        \n",
    "        pc_values = X_test_pca_optimal[:, pc_idx]\n",
    "        feature_values = X_test[:, feat_idx]\n",
    "        \n",
    "        loading = components_df.iloc[feat_idx, pc_idx]\n",
    "        color = 'blue' if loading > 0 else 'red'\n",
    "        \n",
    "        ax.scatter(feature_values, pc_values, alpha=0.5, s=1, c=color)\n",
    "        ax.set_xlabel(f'{feature_names[feat_idx]}')\n",
    "        ax.set_ylabel(f'PC{pc_idx+1}')\n",
    "        ax.set_title(f'Loading: {loading:.3f}')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    for idx in range(n_features, len(axes)):\n",
    "        axes[idx].set_visible(False)\n",
    "    \n",
    "    plt.suptitle(f'Optimal PC{pc_idx+1} vs All Features (explains {pca_optimal.explained_variance_ratio_[pc_idx]:.1%} variance)', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(results_path, f'{results_name}_optimal_PC{pc_idx+1}_vs_all_features.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0ccf7c",
   "metadata": {},
   "source": [
    "## Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57de208d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, latent_dim)\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, input_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        return self.decoder(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db610801",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.from_numpy(X_train.astype(np.float32))\n",
    "X_test_tensor = torch.from_numpy(X_test.astype(np.float32))\n",
    "X_val_tensor = torch.from_numpy(X_val.astype(np.float32))\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(X_train_tensor, X_train_tensor), batch_size=256, shuffle=True)\n",
    "test_loader = DataLoader(TensorDataset(X_test_tensor, X_test_tensor), batch_size=256)\n",
    "val_loader = DataLoader(TensorDataset(X_val_tensor, X_val_tensor), batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b671d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X_train.shape[1]\n",
    "latent_dims = list(range(2, input_dim + 1))\n",
    "\n",
    "ae_results = []\n",
    "training_history = {}\n",
    "optimal_ae_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031fdf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for latent_dim in latent_dims:\n",
    "    print(f\"\\nTraining autoencoder with {latent_dim} latent dimensions...\")\n",
    "    \n",
    "    model = Autoencoder(input_dim, latent_dim).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    epoch_train_losses = []\n",
    "    epoch_val_losses = []\n",
    "    \n",
    "    for epoch in range(100):\n",
    "        model.train()\n",
    "        total_train_loss = 0.0\n",
    "        for batch_x, _ in train_loader:\n",
    "            batch_x = batch_x.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            recon = model(batch_x)\n",
    "            loss = criterion(recon, batch_x)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item() * batch_x.size(0)\n",
    "        \n",
    "        avg_train_loss = total_train_loss / len(train_loader.dataset)\n",
    "        epoch_train_losses.append(avg_train_loss)\n",
    "        \n",
    "        model.eval()\n",
    "        total_val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch_x, _ in val_loader:\n",
    "                batch_x = batch_x.to(device)\n",
    "                recon = model(batch_x)\n",
    "                total_val_loss += criterion(recon, batch_x).item() * batch_x.size(0)\n",
    "        \n",
    "        avg_val_loss = total_val_loss / len(val_loader.dataset)\n",
    "        epoch_val_losses.append(avg_val_loss)\n",
    "        \n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            print(f\"  Epoch {epoch+1}: Train Loss = {avg_train_loss:.6f}, Val Loss = {avg_val_loss:.6f}\")\n",
    "    \n",
    "    training_history[latent_dim] = {\n",
    "        'train': epoch_train_losses,\n",
    "        'val': epoch_val_losses\n",
    "    }\n",
    "    \n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch_x, _ in test_loader:\n",
    "            batch_x = batch_x.to(device)\n",
    "            recon = model(batch_x)\n",
    "            test_loss += criterion(recon, batch_x).item() * batch_x.size(0)\n",
    "    \n",
    "    test_mse = test_loss / len(test_loader.dataset)\n",
    "    \n",
    "    ae_results.append({\n",
    "        'latent_dim': latent_dim,\n",
    "        'train_mse': epoch_train_losses[-1],\n",
    "        'val_mse': epoch_val_losses[-1],\n",
    "        'test_mse': test_mse,\n",
    "        'input_dim': input_dim,\n",
    "        'compression_ratio': input_dim / latent_dim\n",
    "    })\n",
    "    \n",
    "    print(f\"  Test MSE: {test_mse:.6f}\")\n",
    "    \n",
    "    if test_mse < 0.05 and optimal_ae_model is None:\n",
    "        print(f\"  ✓ Found optimal autoencoder: {latent_dim}D with MSE {test_mse:.6f}\")\n",
    "        optimal_ae_latent_dim = latent_dim\n",
    "        optimal_ae_model = model \n",
    "        print(f\"  Saved optimal model, continuing training on remaining dimensions...\")\n",
    "\n",
    "if optimal_ae_model is not None:\n",
    "    print(f\"\\nGenerating embeddings from optimal {optimal_ae_latent_dim}D autoencoder...\")\n",
    "    optimal_ae_model.eval()\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        ae_embeddings = []\n",
    "        for batch_x, _ in test_loader:\n",
    "            batch_x = batch_x.to(device)\n",
    "            embedding = optimal_ae_model.encoder(batch_x).cpu().numpy()\n",
    "            ae_embeddings.append(embedding)\n",
    "        optimal_ae_embedding = np.vstack(ae_embeddings)\n",
    "        \n",
    "        ae_train_embeddings = []\n",
    "        for batch_x, _ in train_loader:\n",
    "            batch_x = batch_x.to(device)\n",
    "            embedding = optimal_ae_model.encoder(batch_x).cpu().numpy()\n",
    "            ae_train_embeddings.append(embedding)\n",
    "        optimal_ae_train_embedding = np.vstack(ae_train_embeddings)\n",
    "        \n",
    "        ae_val_embeddings = []\n",
    "        for batch_x, _ in val_loader:\n",
    "            batch_x = batch_x.to(device)\n",
    "            embedding = optimal_ae_model.encoder(batch_x).cpu().numpy()\n",
    "            ae_val_embeddings.append(embedding)\n",
    "        optimal_ae_val_embedding = np.vstack(ae_val_embeddings)\n",
    "    \n",
    "    print(f\"Generated embeddings for optimal {optimal_ae_latent_dim}D autoencoder\")\n",
    "\n",
    "if 'optimal_ae_embedding' not in locals():\n",
    "    print(\"\\nNo autoencoder achieved MSE < 0.05. Using best performing model...\")\n",
    "    best_ae_result = min(ae_results, key=lambda x: x['test_mse'])\n",
    "    optimal_ae_latent_dim = best_ae_result['latent_dim']\n",
    "    print(f\"Best model: {optimal_ae_latent_dim}D with MSE {best_ae_result['test_mse']:.6f}\")\n",
    "    \n",
    "    model = Autoencoder(input_dim, optimal_ae_latent_dim).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    for epoch in range(100):\n",
    "        model.train()\n",
    "        for batch_x, _ in train_loader:\n",
    "            batch_x = batch_x.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            recon = model(batch_x)\n",
    "            loss = criterion(recon, batch_x)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        ae_embeddings = []\n",
    "        for batch_x, _ in test_loader:\n",
    "            batch_x = batch_x.to(device)\n",
    "            embedding = model.encoder(batch_x).cpu().numpy()\n",
    "            ae_embeddings.append(embedding)\n",
    "        optimal_ae_embedding = np.vstack(ae_embeddings)\n",
    "        \n",
    "        ae_train_embeddings = []\n",
    "        for batch_x, _ in train_loader:\n",
    "            batch_x = batch_x.to(device)\n",
    "            embedding = model.encoder(batch_x).cpu().numpy()\n",
    "            ae_train_embeddings.append(embedding)\n",
    "        optimal_ae_train_embedding = np.vstack(ae_train_embeddings)\n",
    "        \n",
    "        ae_val_embeddings = []\n",
    "        for batch_x, _ in val_loader:\n",
    "            batch_x = batch_x.to(device)\n",
    "            embedding = model.encoder(batch_x).cpu().numpy()\n",
    "            ae_val_embeddings.append(embedding)\n",
    "        optimal_ae_val_embedding = np.vstack(ae_val_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b874d88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nOptimal selections:\")\n",
    "print(f\"PCA: {n_components_95}D (explains {cumulative_variance[n_components_95-1]:.1%} variance)\")\n",
    "print(f\"Autoencoder: {optimal_ae_latent_dim}D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10257926",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ae_results = pd.DataFrame(ae_results)\n",
    "df_ae_results.to_csv(os.path.join(results_path, f'{results_name}_ae_results.csv'), index=False)\n",
    "\n",
    "print(f\"\\nAutoencoder Results Summary:\")\n",
    "print(df_ae_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4a9b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nCreating combined embeddings with optimal dimensions only...\")\n",
    "print(f\"PCA: {n_components_95}D, Autoencoder: {optimal_ae_latent_dim}D\")\n",
    "\n",
    "embeddings_data = []\n",
    "\n",
    "for i in range(len(X_test)):\n",
    "    row_data = {'sample_id': i}\n",
    "    \n",
    "    for j, feature_name in enumerate(feature_names):\n",
    "        row_data[f'feature_{feature_name}'] = X_test[i, j]\n",
    "    \n",
    "    for j in range(n_components_95):\n",
    "        row_data[f'PCA_{j+1}'] = optimal_pca_embedding[i, j]\n",
    "    \n",
    "    for j in range(optimal_ae_latent_dim):\n",
    "        row_data[f'AE_{j+1}'] = optimal_ae_embedding[i, j]\n",
    "    \n",
    "    embeddings_data.append(row_data)\n",
    "\n",
    "df_embeddings = pd.DataFrame(embeddings_data)\n",
    "\n",
    "print(f\"Combined embeddings shape: {df_embeddings.shape}\")\n",
    "print(f\"Features: {len(feature_names)} original + {n_components_95} PCA + {optimal_ae_latent_dim} AE = {len(feature_names) + n_components_95 + optimal_ae_latent_dim} dimensions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870901e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_filename = f'{results_name}_optimal_embeddings.csv'\n",
    "df_embeddings.to_csv(os.path.join(results_path, embeddings_filename), index=False)\n",
    "print(f\"Optimal embeddings saved to: {embeddings_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ab183e",
   "metadata": {},
   "source": [
    "## PCA Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556aaae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_comparison_results = []\n",
    "\n",
    "for n_components in range(2, input_dim + 1):\n",
    "    pca_temp = PCA(n_components=n_components)\n",
    "    pca_temp.fit(X_train)\n",
    "    \n",
    "    X_test_pca_temp = pca_temp.transform(X_test)\n",
    "    X_test_reconstructed = pca_temp.inverse_transform(X_test_pca_temp)\n",
    "    \n",
    "    pca_mse = np.mean((X_test - X_test_reconstructed) ** 2)\n",
    "    explained_var = np.sum(pca_temp.explained_variance_ratio_)\n",
    "    \n",
    "    pca_comparison_results.append({\n",
    "        'n_components': n_components,\n",
    "        'pca_mse': pca_mse,\n",
    "        'explained_variance': explained_var\n",
    "    })\n",
    "\n",
    "df_pca_comparison = pd.DataFrame(pca_comparison_results)\n",
    "df_pca_comparison.to_csv(os.path.join(results_path, f'{results_name}_pca_comparison.csv'), index=False)\n",
    "\n",
    "print(\"PCA Reconstruction Comparison:\")\n",
    "print(df_pca_comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af8b421",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "colors = ['red', 'blue', 'green', 'orange', 'purple', 'brown', 'pink', 'gray', 'olive', 'cyan', 'yellow', 'black', 'white', 'maroon', 'navy', 'lime', 'teal', 'silver', 'gold', 'indigo', 'violet', 'turquoise', 'coral', 'salmon']\n",
    "marker_epochs = [20, 40, 60, 80, 100]\n",
    "\n",
    "filtered_latent_dims = [dim for dim in latent_dims if optimal_ae_latent_dim - 2 <= dim <= optimal_ae_latent_dim + 2]\n",
    "\n",
    "for i, latent_dim in enumerate(filtered_latent_dims): \n",
    "    if latent_dim in training_history:\n",
    "        train_losses = [training_history[latent_dim]['train'][epoch-1] for epoch in marker_epochs]\n",
    "        val_losses = [training_history[latent_dim]['val'][epoch-1] for epoch in marker_epochs]\n",
    "        \n",
    "        linewidth = 3 if latent_dim == optimal_ae_latent_dim else 2\n",
    "        alpha = 1.0 if latent_dim == optimal_ae_latent_dim else 0.7\n",
    "        color = colors[i % len(colors)]\n",
    "        \n",
    "        plt.plot(marker_epochs, train_losses, \n",
    "                 label=f'{latent_dim}D Train', \n",
    "                 color=color, linewidth=linewidth, alpha=alpha, marker='o', markersize=6)\n",
    "        plt.plot(marker_epochs, val_losses, \n",
    "                 label=f'{latent_dim}D Val', \n",
    "                 color=color, linewidth=linewidth, alpha=alpha, marker='s', markersize=6, linestyle='--')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training vs Validation Loss (Optimal ± 2 Models)')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks([20, 40, 60, 80, 100])\n",
    "plt.xlim(15, 105)\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(df_ae_results['latent_dim'], df_ae_results['test_mse'], marker='o', linewidth=2, color='darkblue', markersize=8)\n",
    "plt.axhline(y=0.05, color='red', linestyle='--', alpha=0.7, label='MSE = 0.05 threshold')\n",
    "plt.scatter([optimal_ae_latent_dim], [next(r['test_mse'] for r in ae_results if r['latent_dim'] == optimal_ae_latent_dim)], \n",
    "           color='green', s=100, zorder=5, label=f'Optimal: {optimal_ae_latent_dim}D')\n",
    "plt.xlabel('Latent Dimensions')\n",
    "plt.ylabel('Test MSE')\n",
    "plt.title('Autoencoder Test MSE vs Latent Dimensions')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.xticks(range(int(df_ae_results['latent_dim'].min()), int(df_ae_results['latent_dim'].max()) + 1))\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "x_pos = df_ae_results['latent_dim']\n",
    "width = 0.25\n",
    "\n",
    "bars1 = plt.bar(x_pos - width, df_ae_results['train_mse'], width, label='Train MSE', alpha=0.8, color='lightblue')\n",
    "bars2 = plt.bar(x_pos, df_ae_results['val_mse'], width, label='Val MSE', alpha=0.8, color='orange')  \n",
    "bars3 = plt.bar(x_pos + width, df_ae_results['test_mse'], width, label='Test MSE', alpha=0.8, color='lightgreen')\n",
    "\n",
    "optimal_idx = next(i for i, r in enumerate(ae_results) if r['latent_dim'] == optimal_ae_latent_dim)\n",
    "bars1[optimal_idx].set_edgecolor('black')\n",
    "bars1[optimal_idx].set_linewidth(2)\n",
    "bars2[optimal_idx].set_edgecolor('black') \n",
    "bars2[optimal_idx].set_linewidth(2)\n",
    "bars3[optimal_idx].set_edgecolor('black')\n",
    "bars3[optimal_idx].set_linewidth(2)\n",
    "\n",
    "plt.axhline(y=0.05, color='red', linestyle='--', alpha=0.7, label='MSE = 0.05 threshold')\n",
    "plt.xlabel('Latent Dimensions')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('Train/Val/Test MSE Comparison')\n",
    "plt.legend()\n",
    "plt.xticks(range(int(df_ae_results['latent_dim'].min()), int(df_ae_results['latent_dim'].max()) + 1))\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(df_ae_results['latent_dim'], df_ae_results['test_mse'], 'o-', label='Autoencoder Test MSE', linewidth=2, markersize=8, color='blue')\n",
    "plt.plot(df_pca_comparison['n_components'], df_pca_comparison['pca_mse'], 's-', label='PCA Test MSE', linewidth=2, markersize=8, color='orange')\n",
    "\n",
    "plt.axhline(y=0.05, color='red', linestyle='--', alpha=0.7, label='AE MSE = 0.05 threshold')\n",
    "plt.scatter([n_components_95], [df_pca_comparison[df_pca_comparison['n_components'] == n_components_95]['pca_mse'].iloc[0]], \n",
    "           color='green', s=100, zorder=5, label=f'PCA 95%: {n_components_95}D')\n",
    "plt.scatter([optimal_ae_latent_dim], [next(r['test_mse'] for r in ae_results if r['latent_dim'] == optimal_ae_latent_dim)], \n",
    "           color='purple', s=100, zorder=5, label=f'Optimal AE: {optimal_ae_latent_dim}D')\n",
    "\n",
    "plt.xlabel('Dimensions')\n",
    "plt.ylabel('Test MSE')\n",
    "plt.title('Autoencoder vs PCA Reconstruction Error')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(range(2, input_dim + 1))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(results_path, f'{results_name}_optimal_analysis_plots.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
